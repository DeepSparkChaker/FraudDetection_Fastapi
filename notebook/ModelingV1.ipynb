{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.6.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the librarys\n",
    "import pandas as pd #To work with dataset\n",
    "import numpy as np #Math library\n",
    "import matplotlib.gridspec as gridspec\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import seaborn as sns #Graph library that use matplot in background\n",
    "import matplotlib.pyplot as plt #to plot some parameters in seaborn\n",
    "# Preparation  \n",
    "#import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "# Import StandardScaler from scikit-learn\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer,IterativeImputer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer,ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\n",
    "from sklearn.manifold import TSNE\n",
    "# Import train_test_split()\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import make_scorer,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.metrics import roc_curve,confusion_matrix,classification_report\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime, date\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,LinearRegression\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#import tensorflow as tf \n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "#import smogn\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier,HistGradientBoostingRegressor\n",
    "# For training random forest model\n",
    "import lightgbm as lgb\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans \n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression,f_classif,chi2\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import mutual_info_classif,VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier,LGBMRegressor\n",
    "import lightgbm as lgbm\n",
    "#from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "#from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn import set_config\n",
    "from itertools import combinations\n",
    "import category_encoders as ce\n",
    "#import smong \n",
    "import os \n",
    "import warnings\n",
    "# import optuna \n",
    "from joblib import Parallel, delayed\n",
    "import joblib \n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn import set_config\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Union\n",
    "set_config(display='diagram')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\n"
     ]
    }
   ],
   "source": [
    "!python --version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Model config in json format\"\"\"\n",
    "cfg = {\n",
    "    \"data\": {\n",
    "        ##alldata\n",
    "        \"path\": \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/transactions_train.csv\"\n",
    "        # small sample:\n",
    "       # \"path\": \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/X_train_transactions_train.csv\"\n",
    "    },\n",
    "    # \"data_test\": {\n",
    "    #   \"path\": \"../input/ventilator-pressure-prediction/test.csv\"\n",
    "    # },\n",
    "    # \"data_submission\": {\n",
    "    #   \"path\": \"../input/ventilator-pressure-prediction/test.csv\"\n",
    "    # },\n",
    "    \"train\": {\n",
    "        'fit_params': {'early_stopping_rounds': 100, 'verbose': 55000},\n",
    "        'n_fold': 5,\n",
    "        'seeds': [2021],\n",
    "        'target_col': \"Fraud\",\n",
    "        'debug': False\n",
    "\n",
    "    },\n",
    "    \"model\": {'n_estimators': 11932, \n",
    "                    'max_depth': 16, \n",
    "                    'learning_rate': 0.005352340588475586,\n",
    "                    'lambda_l1': 1.4243404105489683e-06,\n",
    "                    'lambda_l2': 0.04777178032735788,\n",
    "                    'num_leaves': 141, \n",
    "                    'feature_fraction': 0.6657626611307914, \n",
    "                    'bagging_fraction': 0.9115997498937961,\n",
    "                    'bagging_freq': 1,\n",
    "                    'min_child_samples': 51,\n",
    "                     \"objective\": \"binary\",\n",
    "                     #\"metric\": \"binary_logloss\",\n",
    "                     \"verbosity\": -1,\n",
    "                     \"boosting_type\": \"gbdt\",\n",
    "                     #\"random_state\": 228,\n",
    "                     \"metric\": \"auc\",\n",
    "                     #\"device\": \"gpu\",\n",
    "                     'tree_method': \"gpu_hist\"\n",
    "                    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \"\"\"save log\"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.general_logger = logging.getLogger(path)\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n",
    "        if len(self.general_logger.handlers) == 0:\n",
    "            self.general_logger.addHandler(stream_handler)\n",
    "            self.general_logger.addHandler(file_general_handler)\n",
    "            self.general_logger.setLevel(logging.INFO)\n",
    "\n",
    "    def info(self, message):\n",
    "        # display time\n",
    "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
    "\n",
    "    @staticmethod\n",
    "    def now_string():\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    \n",
    "class Util:\n",
    "    \"\"\"save & load\"\"\"\n",
    "    @classmethod\n",
    "    def dump(cls, value, path):\n",
    "        joblib.dump(value, path, compress=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return joblib.load(path)\n",
    "        \n",
    "class HorizontalDisplay:\n",
    "    \"\"\"display dataframe\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        template = '<div style=\"float: left; padding: 10px;\">{0}</div>'\n",
    "        return \"\\n\".join(template.format(arg._repr_html_())\n",
    "                         for arg in self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT = \"../input/ventilator-pressure-prediction\"\n",
    "EXP = \"./\"\n",
    "EXP_MODEL = os.path.join(EXP, \"model\")\n",
    "EXP_FIG = os.path.join(EXP, \"fig\")\n",
    "EXP_PREDS = os.path.join(EXP, \"preds\")\n",
    "\n",
    "# make dirs\n",
    "for d in [EXP_MODEL, EXP_FIG, EXP_PREDS]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# utils\n",
    "logger = Logger(EXP)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Config class\"\"\"\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "class Config:\n",
    "    name_v1 = \"lgb baseline\"\n",
    "    \"\"\"Config class which contains data, train and model hyperparameters\"\"\"\n",
    "    def __init__(self, data, train, model):\n",
    "        self.data = data\n",
    "        self.train = train\n",
    "        self.model = model\n",
    "    @classmethod\n",
    "    def from_json(cls, cfg):\n",
    "        \"\"\"Creates config from json\"\"\"\n",
    "        params = json.loads(json.dumps(cfg), object_hook=lambda d: SimpleNamespace(**d))\n",
    "        return cls(params.data, params.train, params.model)\n",
    "\n",
    "class HelperObject(object):\n",
    "    \"\"\"Helper class to convert json into Python object\"\"\"\n",
    "    def __init__(self, dict_):\n",
    "        self.__dict__.update(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "   Unnamed: 0  step      type     amount     nameOrig  oldbalanceOrig  \\\n",
      "0     5461879   379  CASH_OUT   80762.12   C830155013            0.00   \n",
      "1     3897649   284   CASH_IN  187840.40   C361903179      7304357.50   \n",
      "2     2633725   209   PAYMENT   46846.95  C1930772400       141835.92   \n",
      "3     1264716   134   CASH_IN  570989.27   C419644411        51986.00   \n",
      "4     4730043   332  CASH_OUT   66822.20  C2102103892         7488.00   \n",
      "\n",
      "   newbalanceOrig     nameDest  oldbalanceDest  newbalanceDest  \n",
      "0            0.00   C958383855       180135.78       260897.90  \n",
      "1      7492197.90  C1425876739      6082481.72      5894641.33  \n",
      "2        94988.97  M1132340370            0.00            0.00  \n",
      "3       622975.27    C90786842            0.00            0.00  \n",
      "4            0.00  C1621365985       521335.18       588157.38  \n",
      "shape of data (635119, 10)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Data Loader\"\"\"\n",
    "class DataLoader:\n",
    "    \"\"\"Data Loader class\"\"\"\n",
    "    @staticmethod\n",
    "    def load_data(data_config):\n",
    "        \"\"\"Loads dataset from path\"\"\"\n",
    "        return pd.read_csv(data_config.path)\n",
    "    \n",
    "%time\n",
    "if __name__ == \"__main__\":\n",
    "    train = DataLoader().load_data(Config.from_json(cfg).data)\n",
    "    print(train.head())\n",
    "    print('shape of data {}'.format(train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnsSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, positions):\n",
    "        self.positions = positions\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #return np.array(X)[:, self.positions]\n",
    "        return X.loc[:, self.positions] \n",
    "########################################################################\n",
    "class CustomLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    # https://towardsdatascience.com/how-to-write-powerful-code-others-admire-with-custom-sklearn-transformers-34bc9087fdd\n",
    "    def __init__(self):\n",
    "        self._estimator = PowerTransformer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = np.copy(X) + 1\n",
    "        self._estimator.fit(X_copy)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = np.copy(X) + 1\n",
    "\n",
    "        return self._estimator.transform(X_copy)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X_reversed = self._estimator.inverse_transform(np.copy(X))\n",
    "\n",
    "        return X_reversed - 1  \n",
    "\n",
    "class TemporalVariableTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Temporal elapsed time transformer\n",
    "\n",
    "    def __init__(self, variables, reference_variable):\n",
    "        \n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError('variables should be a list')\n",
    "        \n",
    "        self.variables = variables\n",
    "        self.reference_variable = reference_variable\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we need this step to fit the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "       # so that we do not over-write the original dataframe\n",
    "        X = X.copy()\n",
    "        \n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[self.reference_variable] - X[feature]\n",
    "\n",
    "        return X\n",
    "class CustomImputer(BaseEstimator, TransformerMixin) : \n",
    "    def __init__(self, variable, by) : \n",
    "            #self.something enables you to include the passed parameters\n",
    "            #as object attributes and use it in other methods of the class\n",
    "            self.variable = variable\n",
    "            self.by = by\n",
    "\n",
    "    def fit(self, X, y=None) : \n",
    "        self.map = X.groupby(self.by)[variable].mean()\n",
    "        #self.map become an attribute that is, the map of values to\n",
    "        #impute in function of index (corresponding table, like a dict)\n",
    "        return self\n",
    "\n",
    "def transform(self, X, y=None) : \n",
    "    X[variable] = X[variable].fillna(value = X[by].map(self.map))\n",
    "    #Change the variable column. If the value is missing, value should \n",
    "    #be replaced by the mapping of column \"by\" according to the map you\n",
    "    #created in fit method (self.map)\n",
    "    return X\n",
    "\n",
    "    # categorical missing value imputer\n",
    "class Mapper(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, variables, mappings):\n",
    "\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError('variables should be a list')\n",
    "\n",
    "        self.variables = variables\n",
    "        self.mappings = mappings\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we need the fit statement to accomodate the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[feature].map(self.mappings)\n",
    "\n",
    "        return X  \n",
    "    \n",
    "##########################################################################\n",
    "class CountFrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    #temp = df['card1'].value_counts().to_dict()\n",
    "    #df['card1_counts'] = df['card1'].map(temp)\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoding_method: str = \"count\",\n",
    "        variables: Union[None, int, str, List[Union[str, int]]] = None,\n",
    "        keep_variable=True,\n",
    "                  ) -> None:\n",
    "\n",
    "        self.encoding_method = encoding_method\n",
    "        self.variables = variables\n",
    "        self.keep_variable=keep_variable\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n",
    "        \"\"\"\n",
    "        Learn the counts or frequencies which will be used to replace the categories.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "            The training dataset. Can be the entire dataframe, not just the\n",
    "            variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "            y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        self.encoder_dict_ = {}\n",
    "\n",
    "        # learn encoding maps\n",
    "        for var in self.variables:\n",
    "            if self.encoding_method == \"count\":\n",
    "                self.encoder_dict_[var] = X[var].value_counts().to_dict()\n",
    "\n",
    "            elif self.encoding_method == \"frequency\":\n",
    "                n_obs = float(len(X))\n",
    "                self.encoder_dict_[var] = (X[var].value_counts() / n_obs).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        # replace categories by the learned parameters\n",
    "        X = X.copy()\n",
    "        for feature in self.encoder_dict_.keys():\n",
    "            if self.keep_variable:\n",
    "                X[feature+'_fq_enc'] = X[feature].map(self.encoder_dict_[feature])\n",
    "            else:\n",
    "                X[feature] = X[feature].map(self.encoder_dict_[feature])\n",
    "        return X[self.variables].to_numpy()\n",
    "#################################################   \n",
    "class FeaturesEngineerGroup(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,groupping_method =\"mean\",\n",
    "                   variables=  \"amount\",\n",
    "                   groupby_variables = \"nameOrig\"                         \n",
    "                 ) :\n",
    "        self.groupping_method = groupping_method\n",
    "        self.variables=variables\n",
    "        self.groupby_variables=groupby_variables\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "        The training dataset. Can be the entire dataframe, not just the\n",
    "        variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "        y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        self.group_amount_dict_ = {}\n",
    "        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n",
    "        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
    "        #df = pd.merge(df,temp,on='card1',how='left')\n",
    "        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n",
    "        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n",
    "        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n",
    "        # learn mean/medain \n",
    "        #for groupby in self.groupby_variables:\n",
    "         #   for var in self.variables:\n",
    "        if self.groupping_method == \"mean\":\n",
    "            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['mean']).to_dict()\n",
    "        elif self.groupping_method == \"median\":\n",
    "            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['median']).to_dict()\n",
    "        else:\n",
    "            print('error , chose mean or median')\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        #for col in self.variables:\n",
    "         #   for agg_type in self.groupping_method:\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n",
    "        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_[ self.variables][self.groupping_method])\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)    \n",
    "    \n",
    "################################################   \n",
    "class FeaturesEngineerGroup2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,groupping_method =\"mean\",\n",
    "                   variables=  \"amount\",\n",
    "                   groupby_variables = \"nameOrig\"                         \n",
    "                 ) :\n",
    "        self.groupping_method = groupping_method\n",
    "        self.variables=variables\n",
    "        self.groupby_variables=groupby_variables\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "        The training dataset. Can be the entire dataframe, not just the\n",
    "        variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "        y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        self.group_amount_dict_ = {}\n",
    "        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n",
    "        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
    "        #df = pd.merge(df,temp,on='card1',how='left')\n",
    "        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n",
    "        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n",
    "        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n",
    "        # learn mean/medain \n",
    "        #for groupby in self.groupby_variables:\n",
    "         #   for var in self.variables:\n",
    "\n",
    "        print('we have {} unique clients'.format(X[self.groupby_variables].nunique()))\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method    \n",
    "        X[new_col_name] = X.groupby([self.groupby_variables])[[self.variables]].transform(self.groupping_method)\n",
    "        X = X.drop_duplicates(['nameOrig'])\n",
    "    \n",
    "        self.group_amount_dict_ = dict(zip(X[self.groupby_variables], X[new_col_name]))\n",
    "        del X\n",
    "        #print('we have {} unique mean amount : one for each client'.format(len(self.group_amount_dict_)))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        #for col in self.variables:\n",
    "         #   for agg_type in self.groupping_method:\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n",
    "        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_)\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)   \n",
    "    \n",
    "############################################  \n",
    "class FeaturesEngineerCumCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,group_one =\"step\",\n",
    "                   group_two=  \"nameOrig\"                       \n",
    "                 ) :\n",
    "        self.group_one =group_one\n",
    "        self.group_two=group_two\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        new_col_name =  self.group_two+'_Transaction_count'\n",
    "        X[new_col_name] = X.groupby([self.group_one, self.group_two])[[self.group_two]].transform('count')\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f6d0111e-7c53-4eee-ba3f-704432288751\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f6d0111e-7c53-4eee-ba3f-704432288751\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('cum_count',\n",
       "                                 Pipeline(steps=[('transformer_Encoder',\n",
       "                                                  FeaturesEngineerCumCount())]),\n",
       "                                 ['step', 'nameOrig']),\n",
       "                                ('mean_amount',\n",
       "                                 Pipeline(steps=[('transformer_group_amount_mean',\n",
       "                                                  FeaturesEngineerGroup2()),\n",
       "                                                 ('transformer_group_scaler',\n",
       "                                                  PowerTransformer())]),\n",
       "                                 ['amount', 'nameOrig']),\n",
       "                                ('frequency_dest_orig',\n",
       "                                 Pipeline(steps=[('Encoder',\n",
       "                                                  CountFrequencyEncoder(encoding_method='frequency',\n",
       "                                                                        keep_variable=False,\n",
       "                                                                        variables=['nameOrig',\n",
       "                                                                                   'nameDest']))]),\n",
       "                                 ['nameOrig', 'nameDest']),\n",
       "                                ('trans_type',\n",
       "                                 Pipeline(steps=[('transformer_Encoder',\n",
       "                                                  CatBoostEncoder())]),\n",
       "                                 ['type']),\n",
       "                                ('num',\n",
       "                                 Pipeline(steps=[('scaler',\n",
       "                                                  PowerTransformer())]),\n",
       "                                 ['amount', 'oldbalanceOrig', 'newbalanceOrig',\n",
       "                                  'oldbalanceDest', 'newbalanceDest'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"584c29db-c183-47e2-9889-55e0e6b0dc40\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"584c29db-c183-47e2-9889-55e0e6b0dc40\">cum_count</label><div class=\"sk-toggleable__content\"><pre>['step', 'nameOrig']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2581a49a-9ff7-4995-b22c-cc5f701e7441\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2581a49a-9ff7-4995-b22c-cc5f701e7441\">FeaturesEngineerCumCount</label><div class=\"sk-toggleable__content\"><pre>FeaturesEngineerCumCount()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"42c377c7-cbc3-4969-8284-50405ae143e5\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"42c377c7-cbc3-4969-8284-50405ae143e5\">mean_amount</label><div class=\"sk-toggleable__content\"><pre>['amount', 'nameOrig']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3c2987d3-13e4-4be4-9d2e-e0dd42b7356c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"3c2987d3-13e4-4be4-9d2e-e0dd42b7356c\">FeaturesEngineerGroup2</label><div class=\"sk-toggleable__content\"><pre>FeaturesEngineerGroup2()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2e1de9ef-31ce-4159-9693-e609b80e4f3a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2e1de9ef-31ce-4159-9693-e609b80e4f3a\">PowerTransformer</label><div class=\"sk-toggleable__content\"><pre>PowerTransformer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7eede60c-870f-47aa-a86c-4ee410c90244\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7eede60c-870f-47aa-a86c-4ee410c90244\">frequency_dest_orig</label><div class=\"sk-toggleable__content\"><pre>['nameOrig', 'nameDest']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d3d7c77d-7fa6-4248-bf20-98dd57fa9489\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d3d7c77d-7fa6-4248-bf20-98dd57fa9489\">CountFrequencyEncoder</label><div class=\"sk-toggleable__content\"><pre>CountFrequencyEncoder(encoding_method='frequency', keep_variable=False,\n",
       "                      variables=['nameOrig', 'nameDest'])</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"fc0d8965-a11f-467b-8286-1a067a19f37b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"fc0d8965-a11f-467b-8286-1a067a19f37b\">trans_type</label><div class=\"sk-toggleable__content\"><pre>['type']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3d8f6dd8-514d-4093-a8bf-b5e461054bc6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"3d8f6dd8-514d-4093-a8bf-b5e461054bc6\">CatBoostEncoder</label><div class=\"sk-toggleable__content\"><pre>CatBoostEncoder()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bac4f6ba-7f33-4eca-9d6c-7db010c02c75\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bac4f6ba-7f33-4eca-9d6c-7db010c02c75\">num</label><div class=\"sk-toggleable__content\"><pre>['amount', 'oldbalanceOrig', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a2837453-05b5-4e0e-9d0f-02168a5934a6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a2837453-05b5-4e0e-9d0f-02168a5934a6\">PowerTransformer</label><div class=\"sk-toggleable__content\"><pre>PowerTransformer()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('cum_count',\n",
       "                                 Pipeline(steps=[('transformer_Encoder',\n",
       "                                                  FeaturesEngineerCumCount())]),\n",
       "                                 ['step', 'nameOrig']),\n",
       "                                ('mean_amount',\n",
       "                                 Pipeline(steps=[('transformer_group_amount_mean',\n",
       "                                                  FeaturesEngineerGroup2()),\n",
       "                                                 ('transformer_group_scaler',\n",
       "                                                  PowerTransformer())]),\n",
       "                                 ['amount', 'nameOrig']),\n",
       "                                ('frequency_dest_orig',\n",
       "                                 Pipeline(steps=[('Encoder',\n",
       "                                                  CountFrequencyEncoder(encoding_method='frequency',\n",
       "                                                                        keep_variable=False,\n",
       "                                                                        variables=['nameOrig',\n",
       "                                                                                   'nameDest']))]),\n",
       "                                 ['nameOrig', 'nameDest']),\n",
       "                                ('trans_type',\n",
       "                                 Pipeline(steps=[('transformer_Encoder',\n",
       "                                                  CatBoostEncoder())]),\n",
       "                                 ['type']),\n",
       "                                ('num',\n",
       "                                 Pipeline(steps=[('scaler',\n",
       "                                                  PowerTransformer())]),\n",
       "                                 ['amount', 'oldbalanceOrig', 'newbalanceOrig',\n",
       "                                  'oldbalanceDest', 'newbalanceDest'])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete pipe :\n",
    "# select the float/cat columns\n",
    "#cat_feautres = X.select_dtypes(include=['object','category']).columns\n",
    "#num_features = X.select_dtypes(exclude=['object','category']).columns\n",
    "#Define vcat pipeline\n",
    "features_cum_count=['step','nameOrig']\n",
    "features_groupby_amount=['amount','nameOrig']\n",
    "features_frequency_orig_dest=['nameOrig','nameDest']\n",
    "features_cum_count_pipe = Pipeline([\n",
    "                     ('transformer_Encoder', FeaturesEngineerCumCount())\n",
    "                    ])\n",
    "features_groupby_pipe = Pipeline([\n",
    "                     ('transformer_group_amount_mean', FeaturesEngineerGroup2()),\n",
    "                     ('transformer_group_scaler', PowerTransformer())\n",
    "                    ])\n",
    "features_frequency_pipe = Pipeline([\n",
    "                     ('Encoder', CountFrequencyEncoder(variables=['nameOrig','nameDest'],encoding_method =\"frequency\", keep_variable=False))\n",
    "                    ])\n",
    "type_pipe= Pipeline([\n",
    "                     ('transformer_Encoder', ce.cat_boost.CatBoostEncoder())\n",
    "                    ])\n",
    "num_features0=[  'amount',  'oldbalanceOrig', 'newbalanceOrig' ,'oldbalanceDest', 'newbalanceDest']\n",
    "#Define vnum pipeline\n",
    "num_pipe = Pipeline([\n",
    "                     ('scaler', PowerTransformer()),\n",
    "                    ])\n",
    "#Featureunion fitting training data\n",
    "preprocessor = FeatureUnion(transformer_list=[('cum_count', features_cum_count_pipe),\n",
    "                                              ('mean_amount', features_groupby_pipe),\n",
    "                                              ('frequency_dest_orig', features_frequency_pipe),\n",
    "                                              ('trans_type', type_pipe),\n",
    "                                              ('num', num_pipe)])\n",
    "data_preparing= ColumnTransformer([\n",
    "    ('cum_count', features_cum_count_pipe, features_cum_count ),\n",
    "    ('mean_amount', features_groupby_pipe, features_groupby_amount ),\n",
    "    ('frequency_dest_orig', features_frequency_pipe, features_frequency_orig_dest ),\n",
    "    ('trans_type', type_pipe, ['type'] ),\n",
    "    ('num', num_pipe, num_features0)\n",
    "], remainder='drop')\n",
    "data_preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\rzouga\\\\Desktop\\\\ALLINHERE\\\\ALLINHERE\\\\FraudDetection\\\\DeployPipeComplet\\\\notebook'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrig</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.0</td>\n",
       "      <td>160296.36</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.0</td>\n",
       "      <td>19384.72</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.0</td>\n",
       "      <td>29885.86</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type    amount     nameOrig  oldbalanceOrig  newbalanceOrig  \\\n",
       "0     1   PAYMENT   9839.64  C1231006815        170136.0       160296.36   \n",
       "1     1   PAYMENT   1864.28  C1666544295         21249.0        19384.72   \n",
       "2     1  TRANSFER    181.00  C1305486145           181.0            0.00   \n",
       "3     1  CASH_OUT    181.00   C840083671           181.0            0.00   \n",
       "4     1   PAYMENT  11668.14  C2048537720         41554.0        29885.86   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  \n",
       "0  M1979787155             0.0             0.0        0  \n",
       "1  M2044282225             0.0             0.0        0  \n",
       "2   C553264065             0.0             0.0        1  \n",
       "3    C38997010         21182.0             0.0        1  \n",
       "4  M1230701703             0.0             0.0        0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloder=DataLoader()\n",
    "data = dataloder.load_data(Config.from_json(cfg).data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data for modeling: (2500, 10)\n",
      "Test data for predictions: (2500, 10)\n",
      "Training the model ...\n",
      "we have 2500 unique clients\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6657626611307914, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6657626611307914\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4243404105489683e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4243404105489683e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9115997498937961, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9115997498937961\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04777178032735788, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04777178032735788\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Testing the model ...\n",
      "auc_test: 0.9966343564889607\n",
      "Model saved at C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/models/model_test.joblib\n"
     ]
    }
   ],
   "source": [
    "# Importing the required Python libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "#from configs_json import cfg\n",
    "#from utils.config import  Config\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from dataloader.dataloader  import  DataLoader\n",
    "\n",
    "def create_train_test_data(dataset,train_size=0.5):\n",
    "    # load and split the data\n",
    "    data_train = dataset.sample(frac=train_size, random_state=30).reset_index(drop=True)\n",
    "    data_test = dataset.drop(data_train.index).reset_index(drop=True)\n",
    "    # save the data\n",
    "    data_train.to_csv('C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/data/train.csv', index=False)\n",
    "    data_test.to_csv('C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/data/test.csv', index=False)\n",
    "    print(f\"Train data for modeling: {data_train.shape}\")\n",
    "    print(f\"Test data for predictions: {data_test.shape}\")\n",
    "\n",
    "def train_model(x_train, y_train):\n",
    "     \n",
    "    print(\"Training the model ...\")\n",
    "    # Model defined \n",
    "    params={\"learning_rate\": Config.from_json(cfg).model.learning_rate,\n",
    "        #'device': Config.from_json(cfg).model.device,\n",
    "        'metric':Config.from_json(cfg).model.metric,\n",
    "        'objective':Config.from_json(cfg).model.objective,\n",
    "        'n_estimators':  Config.from_json(cfg).model.n_estimators,\n",
    "        'num_leaves': Config.from_json(cfg).model.num_leaves,\n",
    "        'min_child_samples':  Config.from_json(cfg).model.min_child_samples,\n",
    "        'feature_fraction': Config.from_json(cfg).model.feature_fraction,\n",
    "        'bagging_fraction': Config.from_json(cfg).model.bagging_fraction,\n",
    "        'bagging_freq':  Config.from_json(cfg).model.bagging_freq,\n",
    "        #'reg_alpha':  Config.from_json(cfg).model.reg_alpha,\n",
    "        #'reg_lambda':  Config.from_json(cfg).model.reg_lambda,\n",
    "       # 'gpu_platform_id':  Config.from_json(cfg).model.gpu_platform_id\n",
    "            }\n",
    "    params_optuna={'n_estimators': 11932, \n",
    "                    'max_depth': 16, \n",
    "                    'learning_rate': 0.005352340588475586,\n",
    "                    'lambda_l1': 1.4243404105489683e-06,\n",
    "                    'lambda_l2': 0.04777178032735788,\n",
    "                    'num_leaves': 141, \n",
    "                    'feature_fraction': 0.6657626611307914, \n",
    "                    'bagging_fraction': 0.9115997498937961,\n",
    "                    'bagging_freq': 1,\n",
    "                    'min_child_samples': 51,\n",
    "                     \"objective\": \"binary\",\n",
    "                     #\"metric\": \"binary_logloss\",\n",
    "                     \"verbosity\": -1,\n",
    "                     \"boosting_type\": \"gbdt\",\n",
    "                     #\"random_state\": 228,\n",
    "                     \"metric\": \"auc\",\n",
    "                     #\"device\": \"gpu\",\n",
    "                     'tree_method': \"gpu_hist\"\n",
    "                    }\n",
    "    model_lgbm  = LGBMRegressor(**params_optuna)\n",
    "    # Pipline preprocess\n",
    "    pipeline_model_lgbm = Pipeline([ \n",
    "        ('pre', data_preparing),\n",
    "        ('lgbm', model_lgbm)\n",
    "    ])\n",
    "    pipeline_model_lgbm.fit(x_train, y_train)\n",
    "\n",
    "    return pipeline_model_lgbm\n",
    "\n",
    "def accuracy(model, x_test, y_test):\n",
    "    print(\"Testing the model ...\")\n",
    "    predictions = model.predict(x_test)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    return roc_auc\n",
    "\n",
    "def export_model(model):\n",
    "    # Save the model\n",
    "    joblib_path = 'C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/models/model_test.joblib'\n",
    "    with open(joblib_path, 'wb') as file:\n",
    "        joblib.dump(model, file)\n",
    "        print(f\"Model saved at {joblib_path}\")\n",
    "\n",
    "def main():\n",
    "    #mlops data \n",
    "    # data =\n",
    "    # Load the whole data\n",
    "    dataloder=DataLoader()\n",
    "    data = dataloder.load_data(Config.from_json(cfg).data)\n",
    "  \n",
    "    # Split train/test\n",
    "    # Creates train.csv and test.csv\n",
    "    create_train_test_data(data.iloc[0:5000,:],train_size=0.5)\n",
    "    # Loads the data for the model training\n",
    "    train = pd.read_csv('C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/data/train.csv', keep_default_na=False)\n",
    "    #num_columns=train.drop(['isFraud'], axis=1).select_dtypes(include=['int64','float64']).columns\n",
    "    #cat_columns= train.drop(['isFraud'], axis=1).select_dtypes(exclude=['int64','float64']).columns\n",
    "    x_train = train.drop(['isFraud'], axis=1)\n",
    "    y_train =  train['isFraud'].to_numpy()\n",
    "     \n",
    "    # Loads the data for the model testing\n",
    "    test = pd.read_csv('C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/data/test.csv', keep_default_na=False)\n",
    "    x_test = test.drop(['isFraud'], axis=1)\n",
    "    y_test = test['isFraud'].to_numpy()\n",
    "\n",
    "    # Train and Test\n",
    "    model = train_model(x_train, y_train)\n",
    "    auc_test = accuracy(model, x_test, y_test)\n",
    "   \n",
    "    print(f\"auc_test: {auc_test}\")\n",
    "\n",
    "    # Save the model\n",
    "    export_model(model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Fraud\n"
     ]
    }
   ],
   "source": [
    "#load model data\n",
    "f = \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/models/model_test.joblib\"\n",
    "model = joblib.load(f)\n",
    "item={}\n",
    "item={\"step\":234,\n",
    "       \"type\":\"CASH_OUT\",\n",
    "       \"amount\":305822.52,\n",
    "       \"nameOrig\":\"C1376293938\",\n",
    "       \"oldbalanceOrig\":0.0,\n",
    "       \"newbalanceOrig\":0.0,\n",
    "       \"nameDest\":\"C182325611\",\n",
    "       \"oldbalanceDest\":1569390.8999999999,\n",
    "       \"newbalanceDest\":1875213.4199999999}\n",
    "df = pd.json_normalize(item)\n",
    "df2 =pd.DataFrame([item])\n",
    "#data = test1.dict()\n",
    "#step=data['step']\n",
    "#type=data['type']\n",
    "#amount=data['amount']\n",
    "#nameOrig=data['nameOrig']\n",
    "#oldbalanceOrig=data['oldbalanceOrig']\n",
    "#newbalanceOrig=data['newbalanceOrig']\n",
    "#oldbalanceDest=data['oldbalanceDest']\n",
    "#newbalanceDest=data['newbalanceDest']\n",
    "# print(classifier.predict([[variance,skewness,curtosis,entropy]]))\n",
    "#prediction = classifier.predict([[variance,skewness,curtosis,entropy]])\n",
    "# Predicting Test Set\n",
    "test2={\"step\":item[\"step\"],\n",
    "       \"type\":item[\"type\"],\n",
    "       \"amount\":item[\"amount\"],\n",
    "       \"nameOrig\":item[\"nameOrig\"],\n",
    "       \"oldbalanceOrig\":item[\"oldbalanceOrig\"],\n",
    "       \"newbalanceOrig\":item[\"newbalanceOrig\"],\n",
    "       \"nameDest\":item[\"nameDest\"],\n",
    "       \"oldbalanceDest\":item[\"oldbalanceDest\"],\n",
    "       \"newbalanceDest\":item[\"newbalanceDest\"]}\n",
    "\n",
    "data_dict = pd.DataFrame(\n",
    "        {\n",
    "            'step': [item[\"step\"]],\n",
    "            'type': [item[\"type\"]],\n",
    "            'amount': [item[\"amount\"]],\n",
    "            'nameOrig': [item[\"nameOrig\"]],\n",
    "            'oldbalanceOrig': [item['oldbalanceOrig']],\n",
    "            'newbalanceOrig': [item[\"newbalanceOrig\"]],\n",
    "            'nameDest': [item[\"nameDest\"]],\n",
    "            'oldbalanceDest': [item[\"oldbalanceDest\"]],\n",
    "            'newbalanceDest': [item[\"newbalanceDest\"]]\n",
    "        }\n",
    "    )\n",
    "prediction = model.predict(data_dict)\n",
    "if (prediction[0] > 0.5):\n",
    "    prediction = \"Fraud\"\n",
    "else:\n",
    "    prediction = \"Not Fraud\"\n",
    "print(prediction)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [4184]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:4000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# First, we will need to import the library and initialize the main application object:\n",
    "import joblib\n",
    "import uvicorn\n",
    "from fastapi import FastAPI,Request, File, UploadFile, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nest_asyncio\n",
    "from typing import Any, Dict,List\n",
    "\n",
    "        \n",
    "## API INSTANTIATION\n",
    "## ----------------------------------------------------------------\n",
    "       \n",
    "app = FastAPI(\n",
    "    title=\"Fraud Detection API\",\n",
    "    description=\"A simple API that use Ml model to predict fraud \",\n",
    "    version=\"0.1\",\n",
    ")\n",
    "# Creating the data model for data validation\n",
    "class ClientData(BaseModel):\n",
    "    step: List[int]\n",
    "    type: List[str]\n",
    "    amount: List[float]\n",
    "    nameOrig:  List[str]\n",
    "    oldbalanceOrig: List[float]\n",
    "    newbalanceOrig: List[float]\n",
    "    nameDest:  List[str]\n",
    "    oldbalanceDest: List[float]\n",
    "    newbalanceDest: List[float]\n",
    "\n",
    "#load model data\n",
    "f = \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/models/model_test.joblib\"\n",
    "#f = \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/models/pipeline_model_lgbm_final.joblib\"\n",
    "model = joblib.load(f)\n",
    "    \n",
    "## API ENDPOINTS\n",
    "## ----------------------------------------------------------------\n",
    "## API ENDPOINTS\n",
    "## ----------------------------------------------------------------\n",
    "\n",
    "##################\n",
    "@app.get('/')\n",
    "def index():\n",
    "  '''\n",
    "  This is a first docstring.\n",
    "  '''\n",
    "  return {'message': 'This is a Fraud  Classification API!'}\n",
    "\n",
    "# Tester\n",
    "@app.get('/ping')\n",
    "def ping():\n",
    "  '''\n",
    "  This is a first docstring.\n",
    "  '''\n",
    "  return ('pong', 200)\n",
    "# Defining the prediction endpoint without data validation\n",
    "@app.post('/basic_predict')\n",
    "async def basic_predict(request: Request):\n",
    "    '''\n",
    "    This is a first docstring.\n",
    "    '''\n",
    "    # Getting the JSON from the body of the request\n",
    "    input_data = await request.json()\n",
    "\n",
    "    # Converting JSON to Pandas DataFrame\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "\n",
    "    # Getting the prediction \n",
    "    pred = model.predict(input_df)[0]\n",
    "\n",
    "    return pred\n",
    "\n",
    "# We now define the function that will be executed for each URL request and return the value:\n",
    "@app.post(\"/predict-fraud\")\n",
    "async  def predict_fraud(item :ClientData):\n",
    "    \"\"\"\n",
    "    A simple function that receive a client data and predict Fraud.\n",
    "    :param client_data:\n",
    "    :return: prediction, probabilities\n",
    "\n",
    "    \"\"\"\n",
    "    # perform prediction\n",
    "    #df =pd.DataFrame([item])\n",
    "    h=item.dict()\n",
    "    df=pd.DataFrame.from_dict(h, orient=\"columns\")\n",
    "    prediction = model.predict(df)\n",
    "    prediction_final=[\"Fraud\" if (x > 0.5) else \"Not Fraud\" for x in prediction ]\n",
    "    return prediction_final\n",
    "    \n",
    "    # Create the POST endpoint with path '/predict'\n",
    "@app.post(\"/predict_csv\")\n",
    "async def create_upload_file(file: UploadFile = File(...)):\n",
    "    # Handle the file only if it is a CSV\n",
    "    if file.filename.endswith(\".csv\"):\n",
    "        # Create a temporary file with the same name as the uploaded \n",
    "        # CSV file to load the data into a pandas Dataframe\n",
    "        with open(file.filename, \"wb\")as f:\n",
    "            f.write(file.file.read())\n",
    "        data = pd.read_csv(file.filename)\n",
    "        os.remove(file.filename)\n",
    "        # Return a JSON object containing the model predictions\n",
    "        return {\n",
    "            \"predictions\": model.predict(data)\n",
    "        }    \n",
    "    else:\n",
    "        # Raise a HTTP 400 Exception, indicating Bad Request \n",
    "        # (you can learn more about HTTP response status codes here)\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid file format. Only CSV Files accepted.\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=4000)\n",
    "# uvicorn app:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrig</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>string</td>\n",
       "      <td>0.00</td>\n",
       "      <td>string</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>string</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>305822.52</td>\n",
       "      <td>C1376293938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C182325611</td>\n",
       "      <td>1569390.9</td>\n",
       "      <td>1875213.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type     amount     nameOrig  oldbalanceOrig  newbalanceOrig  \\\n",
       "0     0    string       0.00       string             0.0             0.0   \n",
       "1   234  CASH_OUT  305822.52  C1376293938             0.0             0.0   \n",
       "\n",
       "     nameDest  oldbalanceDest  newbalanceDest  \n",
       "0      string             0.0            0.00  \n",
       "1  C182325611       1569390.9      1875213.42  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item={\n",
    "  \"step\": [\n",
    "    0,234\n",
    "  ],\n",
    "  \"type\": [\n",
    "    \"string\",\"CASH_OUT\"\n",
    "  ],\n",
    "  \"amount\": [\n",
    "    0,305822.52\n",
    "  ],\n",
    "  \"nameOrig\": [\n",
    "    \"string\",\"C1376293938\"\n",
    "  ],\n",
    "  \"oldbalanceOrig\": [\n",
    "    0,0.0\n",
    "  ],\n",
    "  \"newbalanceOrig\": [\n",
    "    0,0.0\n",
    "  ],\n",
    "  \"nameDest\": [\n",
    "    \"string\",\"C182325611\"\n",
    "  ],\n",
    "  \"oldbalanceDest\": [\n",
    "    0,1569390.8999999999\n",
    "  ],\n",
    "  \"newbalanceDest\": [\n",
    "    0,1875213.4199999999\n",
    "  ]\n",
    "}\n",
    "df=pd.DataFrame.from_dict(item, orient=\"columns\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not Fraud', 'Not Fraud']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(df)\n",
    "prediction_final=[\"Fraud\" if (x > 0.5) else \"Not Fraud\" for x in prediction]\n",
    "prediction_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting session_info\n",
      "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
      "Collecting stdlib_list\n",
      "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
      "Building wheels for collected packages: session-info\n",
      "  Building wheel for session-info (setup.py): started\n",
      "  Building wheel for session-info (setup.py): finished with status 'done'\n",
      "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8053 sha256=75caa36a854071fb2e3a2768f884be3c98798334c0fd505d71391f1dd74a940d\n",
      "  Stored in directory: c:\\users\\rzouga\\appdata\\local\\pip\\cache\\wheels\\5c\\1b\\4d\\111d73980c5c6a8e5e5905a19eccc34296cb359cac54c6c5b9\n",
      "Successfully built session-info\n",
      "Installing collected packages: stdlib-list, session-info\n",
      "Successfully installed session-info-1.0.0 stdlib-list-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install session_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "category_encoders   2.3.0\n",
       "fastapi             0.65.0\n",
       "joblib              1.0.1\n",
       "lightgbm            3.3.1\n",
       "matplotlib          3.3.4\n",
       "nest_asyncio        NA\n",
       "numpy               1.20.1\n",
       "pandas              1.2.4\n",
       "plotly              5.4.0\n",
       "pydantic            NA\n",
       "scipy               1.6.2\n",
       "seaborn             0.11.1\n",
       "session_info        1.0.0\n",
       "sklearn             0.24.1\n",
       "starlette           0.14.2\n",
       "tqdm                4.59.0\n",
       "uvicorn             0.15.0\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "Cython              0.29.23\n",
       "PIL                 8.2.0\n",
       "anyio               NA\n",
       "asgiref             3.4.1\n",
       "attr                20.3.0\n",
       "babel               2.9.0\n",
       "backcall            0.2.0\n",
       "bottleneck          1.3.2\n",
       "brotli              NA\n",
       "certifi             2020.12.05\n",
       "cffi                1.14.5\n",
       "chardet             4.0.0\n",
       "click               7.1.2\n",
       "cloudpickle         1.6.0\n",
       "colorama            0.4.4\n",
       "cycler              0.10.0\n",
       "cython              0.29.23\n",
       "cython_runtime      NA\n",
       "cytoolz             0.11.0\n",
       "dask                2021.04.0\n",
       "dateutil            2.8.1\n",
       "decorator           5.0.6\n",
       "distributed         2021.04.0\n",
       "fsspec              0.9.0\n",
       "h11                 0.12.0\n",
       "idna                2.10\n",
       "ipykernel           5.3.4\n",
       "ipython_genutils    0.2.0\n",
       "ipywidgets          7.6.3\n",
       "jedi                0.17.2\n",
       "jinja2              2.11.3\n",
       "json5               NA\n",
       "jsonschema          3.2.0\n",
       "jupyter_server      1.4.1\n",
       "jupyterlab_server   2.4.0\n",
       "kiwisolver          1.3.1\n",
       "markupsafe          1.1.1\n",
       "mkl                 2.3.0\n",
       "mpl_toolkits        NA\n",
       "msgpack             1.0.2\n",
       "multipart           0.0.5\n",
       "nbclassic           NA\n",
       "nbformat            5.1.3\n",
       "nt                  NA\n",
       "ntsecuritycon       NA\n",
       "numexpr             2.7.3\n",
       "packaging           20.9\n",
       "parso               0.7.0\n",
       "patsy               0.5.1\n",
       "pickleshare         0.7.5\n",
       "pkg_resources       NA\n",
       "prometheus_client   NA\n",
       "prompt_toolkit      3.0.17\n",
       "psutil              5.8.0\n",
       "pvectorc            NA\n",
       "pygments            2.8.1\n",
       "pyparsing           2.4.7\n",
       "pyrsistent          NA\n",
       "pythoncom           NA\n",
       "pytz                2021.1\n",
       "pywintypes          NA\n",
       "requests            2.25.1\n",
       "send2trash          NA\n",
       "six                 1.15.0\n",
       "skimage             0.18.1\n",
       "sniffio             1.2.0\n",
       "socks               1.7.1\n",
       "sortedcontainers    2.3.0\n",
       "sphinxcontrib       NA\n",
       "statsmodels         0.12.2\n",
       "storemagic          NA\n",
       "tblib               1.7.0\n",
       "tenacity            NA\n",
       "threadpoolctl       2.1.0\n",
       "tlz                 0.11.0\n",
       "toolz               0.11.1\n",
       "tornado             6.1\n",
       "traitlets           5.0.5\n",
       "typing_extensions   NA\n",
       "ujson               4.0.2\n",
       "urllib3             1.26.4\n",
       "wcwidth             0.2.5\n",
       "win32api            NA\n",
       "win32com            NA\n",
       "win32security       NA\n",
       "yaml                5.4.1\n",
       "zmq                 20.0.0\n",
       "zope                NA\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             7.22.0\n",
       "jupyter_client      6.1.12\n",
       "jupyter_core        4.7.1\n",
       "jupyterlab          3.0.14\n",
       "notebook            6.3.0\n",
       "-----\n",
       "Python 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-7-6.1.7601-SP1\n",
       "-----\n",
       "Session information updated at 2021-12-06 22:00\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
    "   File \"/app/main.py\", line 51, in <module>\n",
    "\n",
    "    model = joblib.load(joblib_filename)\n",
    "\n",
    "  File \"/app/.heroku/python/lib/python3.7/site-packages/joblib/numpy_pickle.py\", line 577, in load\n",
    "\n",
    "     obj = _unpickle(fobj)\n",
    "\n",
    "  File \"/app/.heroku/python/lib/python3.7/site-packages/joblib/numpy_pickle.py\", line 506, in _unpickle\n",
    "\n",
    "    obj = unpickler.load()\n",
    "\n",
    "   File \"/app/.heroku/python/lib/python3.7/pickle.py\", line 1088, in load\n",
    "\n",
    "     dispatch[key[0]](self)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
